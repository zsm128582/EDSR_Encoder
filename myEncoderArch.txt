myEncoder(
  (encoder): EDSR(
    (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
    (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
    (head): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (body): Sequential(
      (0): SelfAttention(
        (sa): SimpleAttention(
          (linears): ModuleList(
            (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)
          )
          (norm_K): ModuleList(
            (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
          )
          (norm_V): ModuleList(
            (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (posEmbedding): Positional encoding PositionEmbeddingSine
            num_pos_feats: 32
            temperature: 10000
            normalize: True
            scale: 6.283185307179586
        (dropout): Dropout(p=0.1, inplace=False)
        (layerNorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (ffn): FFNLayer(
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (3): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (4): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (5): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (6): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (7): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (8): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (9): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (10): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (11): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (12): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (13): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (14): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (15): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (16): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
  )
  (sa1): SelfAttention(
    (sa): SimpleAttention(
      (linears): ModuleList(
        (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)
      )
      (norm_K): ModuleList(
        (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (norm_V): ModuleList(
        (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (posEmbedding): Positional encoding PositionEmbeddingSine
        num_pos_feats: 32
        temperature: 10000
        normalize: True
        scale: 6.283185307179586
    (dropout): Dropout(p=0.1, inplace=False)
    (layerNorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (ffn): FFNLayer(
      (linear1): Linear(in_features=64, out_features=128, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=128, out_features=64, bias=True)
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sa2): SelfAttention(
    (sa): SimpleAttention(
      (linears): ModuleList(
        (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)
      )
      (norm_K): ModuleList(
        (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (norm_V): ModuleList(
        (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (posEmbedding): Positional encoding PositionEmbeddingSine
        num_pos_feats: 32
        temperature: 10000
        normalize: True
        scale: 6.283185307179586
    (dropout): Dropout(p=0.1, inplace=False)
    (layerNorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (ffn): FFNLayer(
      (linear1): Linear(in_features=64, out_features=128, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=128, out_features=64, bias=True)
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=3, bias=True)
    )
  )
  (decoder): myDecoder(
    (decoder_blocks): ModuleList(
      (0-3): 4 x SelfAttention(
        (sa): SimpleAttention(
          (linears): ModuleList(
            (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)
          )
          (norm_K): ModuleList(
            (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
          )
          (norm_V): ModuleList(
            (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (posEmbedding): Positional encoding PositionEmbeddingSine
            num_pos_feats: 32
            temperature: 10000
            normalize: True
            scale: 6.283185307179586
        (dropout): Dropout(p=0.1, inplace=False)
        (layerNorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (ffn): FFNLayer(
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (decoder_pred): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): Linear(in_features=128, out_features=3, bias=True)
      )
    )
    (patchEmbed): PatchEmbed(
      (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
      (norm): Identity()
    )
    (blocks): ModuleList(
      (0-1): 2 x Block(
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
    )
    (patchNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (patch_pred): Linear(in_features=1024, out_features=768, bias=True)
  )
)