config loaded.
train dataset: size=100000
val dataset: size=20580
model: #params=592.1M
DataParallel(
  (module): myEncoder(
    (encoder): EDSR(
      (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
      (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
      (head): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (body): Sequential(
        (0): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (1): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (2): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (3): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (4): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (5): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (6): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (7): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (8): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (9): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (10): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (11): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (12): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (13): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (14): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (15): ResBlock(
          (sa1): SimpleAttention(
            (linears): ModuleList(
              (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=True)
            )
            (norm_K): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm_V): ModuleList(
              (0-7): 8 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (body): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (patch_embed): PatchEmbed(
            (proj): Conv2d(64, 1024, kernel_size=(16, 16), stride=(16, 16))
            (norm): Identity()
          )
          (rebuild): Linear(in_features=1024, out_features=16384, bias=True)
        )
        (16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (sa1): SimpleAttention(
      (linears): ModuleList(
        (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)
      )
      (norm_K): ModuleList(
        (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (norm_V): ModuleList(
        (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (sa2): SimpleAttention(
      (linears): ModuleList(
        (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)
      )
      (norm_K): ModuleList(
        (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (norm_V): ModuleList(
        (0-7): 8 x LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (posEmbedding): Positional encoding PositionEmbeddingSine
        num_pos_feats: 32
        temperature: 10000
        normalize: True
        scale: 6.283185307179586
    (dropout): Dropout(p=0.1, inplace=False)
    (layerNorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (ffn): FFNLayer(
      (linear1): Linear(in_features=64, out_features=128, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=128, out_features=64, bias=True)
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (mlp): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=64, out_features=64, bias=True)
        (2): Linear(in_features=64, out_features=3, bias=True)
      )
    )
  )
)
